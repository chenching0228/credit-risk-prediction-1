{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd90e36-108d-440e-b3eb-d86030119a1a",
   "metadata": {},
   "source": [
    "## Feature Engineering Refinement\n",
    "Utilise the comprehensive feature analysis from feature_engineering to select optimal feature sets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4237c146-3f5b-4bd4-b1e7-5d0d3d897f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available feature sets:\n",
      "  minimal_set: 3 features\n",
      "  recommended_set: 4 features\n",
      "  comprehensive_set: 4 features\n",
      "  tier1_features: 3 features\n",
      "  tier2_features: 1 features\n",
      "  tier3_features: 0 features\n",
      "  tier4_features: 22 features\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# load feature importance rankings\n",
    "processed_dir = Path('data/processed')\n",
    "importance_df = pd.read_csv('../data/processed/feature_importance_analysis.csv')\n",
    "\n",
    "# load selected feature sets\n",
    "with open(('../data/processed/selected_feature_sets.json'), 'r') as f:\n",
    "    feature_sets = json.load(f)\n",
    "\n",
    "print(\"available feature sets:\")\n",
    "for set_name, features in feature_sets.items():\n",
    "    print(f\"  {set_name}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd83e89-16ef-499a-ad96-43ba53b81df5",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38c7d8b-baf5-475a-b00f-32eb79cedaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 211,610 samples\n",
      "validation set: 45,345 samples\n",
      "test set: 45,346 samples\n",
      "features: 26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 2025\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def prepare_modelling_data(df: pd.DataFrame, target: str = 'target') -> Tuple:\n",
    "    \"\"\"\n",
    "    prepare data for model training with proper encoding and splitting\n",
    "    \n",
    "    args:\n",
    "        df: preprocessed dataframe\n",
    "        target: target variable name\n",
    "        \n",
    "    returns:\n",
    "        tuple of (X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders)\n",
    "    \"\"\"\n",
    "    df_model_prep = df.copy()\n",
    "    \n",
    "    # separate features and target\n",
    "    if target not in df_model_prep.columns:\n",
    "        raise ValueError(f\"target column '{target}' not found in dataframe\")\n",
    "    \n",
    "    y = df_model_prep[target]\n",
    "    X = df_model_prep.drop(columns=[target])\n",
    "    \n",
    "    # encode categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    # convert datetime columns to numeric\n",
    "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    for col in datetime_cols:\n",
    "        X[f'{col}_year'] = X[col].dt.year\n",
    "        X[f'{col}_month'] = X[col].dt.month\n",
    "        X = X.drop(columns=[col])\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Set your upper and lower bounds\n",
    "    lower_bound = -1e6\n",
    "    upper_bound = 1e6\n",
    "\n",
    "    # Apply across all numeric columns\n",
    "    X = X.clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    # split data: 70% train, 15% validation, 15% test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"train set: {len(X_train):,} samples\")\n",
    "    print(f\"validation set: {len(X_val):,} samples\")\n",
    "    print(f\"test set: {len(X_test):,} samples\")\n",
    "    print(f\"features: {len(feature_names)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders\n",
    "\n",
    "# load data\n",
    "df_model_processed = pd.read_csv(\"/Users/chenjing/Desktop/credit-risk-prediction/data/processed/accepted_loans_model_ready.csv\")\n",
    "\n",
    "\n",
    "# prepare data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders = prepare_modelling_data(\n",
    "    df_model_processed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b810748-cc9a-4286-832f-8945add0fb97",
   "metadata": {},
   "source": [
    "## Feature Set Comparison Strategy\n",
    "Systematically compare model performance across different feature sets to identify optimal feature configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008464ee-43de-496f-add6-237552d5a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal_set: 0.6947 ± 0.0013\n",
      "recommended_set: 0.7000 ± 0.0013\n",
      "comprehensive_set: 0.7000 ± 0.0013\n",
      "tier1_features: 0.6947 ± 0.0013\n",
      "tier2_features: 0.5947 ± 0.0015\n",
      "skipping tier3_features: no features available\n",
      "tier4_features: 0.6656 ± 0.0038\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>n_features</th>\n",
       "      <th>mean_auc</th>\n",
       "      <th>std_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recommended_set</td>\n",
       "      <td>4</td>\n",
       "      <td>0.699953</td>\n",
       "      <td>0.001315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comprehensive_set</td>\n",
       "      <td>4</td>\n",
       "      <td>0.699953</td>\n",
       "      <td>0.001315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minimal_set</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694675</td>\n",
       "      <td>0.001301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tier1_features</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694675</td>\n",
       "      <td>0.001301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tier4_features</td>\n",
       "      <td>22</td>\n",
       "      <td>0.665602</td>\n",
       "      <td>0.003757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tier2_features</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594736</td>\n",
       "      <td>0.001476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_set  n_features  mean_auc   std_auc\n",
       "1    recommended_set           4  0.699953  0.001315\n",
       "2  comprehensive_set           4  0.699953  0.001315\n",
       "0        minimal_set           3  0.694675  0.001301\n",
       "3     tier1_features           3  0.694675  0.001301\n",
       "5     tier4_features          22  0.665602  0.003757\n",
       "4     tier2_features           1  0.594736  0.001476"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "def compare_feature_sets(X_train: pd.DataFrame, y_train: pd.Series, \n",
    "                        feature_sets: Dict[str, List[str]], \n",
    "                        cv: int = 5, \n",
    "                        random_state: int = 2025) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    evaluate model performance across different feature sets using cross-validation\n",
    "    \n",
    "    args:\n",
    "        X_train: training features dataframe\n",
    "        y_train: training target series\n",
    "        feature_sets: dictionary mapping set names to feature lists\n",
    "        cv: number of cross-validation folds (default 5)\n",
    "        random_state: random seed for reproducibility\n",
    "        \n",
    "    returns:\n",
    "        dataframe with mean and std auc scores for each feature set\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for set_name, features in feature_sets.items():\n",
    "        # filter to available features in X_train\n",
    "        available_features = [f for f in features if f in X_train.columns]\n",
    "        \n",
    "        if len(available_features) == 0:\n",
    "            print(f\"skipping {set_name}: no features available\")\n",
    "            continue\n",
    "            \n",
    "        # create baseline model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        # cross-validation\n",
    "        scores = cross_val_score(\n",
    "            model, \n",
    "            X_train[available_features], \n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'feature_set': set_name,\n",
    "            'n_features': len(available_features),\n",
    "            'mean_auc': scores.mean(),\n",
    "            'std_auc': scores.std()\n",
    "        })\n",
    "        \n",
    "        print(f\"{set_name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('mean_auc', ascending=False)\n",
    "\n",
    "# execute comparison\n",
    "feature_set_results = compare_feature_sets(X_train, y_train, feature_sets, cv=5)\n",
    "feature_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8407b-b63a-4710-acb5-7f9923e532d1",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation\n",
    "| Method | Mechanism | Advantages | Disadvantages | Use When |\n",
    "|--------|-----------|------------|---------------|----------|\n",
    "| **Grid Search** | exhaustive search over specified parameter grid | guaranteed to find best combination in grid | computationally expensive, curse of dimensionality | small parameter space, need comprehensive search |\n",
    "| **Random Search** | random sampling from parameter distributions | more efficient than grid for large spaces | may miss optimal combination | large parameter space, limited compute budget |\n",
    "| **Bayesian Optimisation** | builds probabilistic model of objective function | efficient for expensive evaluations | complex implementation, requires careful tuning | expensive model training, need sample efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bb3a1-ad90-4673-8b93-4fd3a9e0dbad",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Tuning\n",
    "Optimise random forest using randomised search for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5fb3d95-6dfb-4bac-a62d-d1dff6d3d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting randomised search with 10 iterations...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "best auc: 0.7191\n",
      "best parameters: {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 41, 'min_samples_split': 114, 'n_estimators': 473}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "def tune_random_forest(X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                       n_iter: int = 50,\n",
    "                       cv: int = 5,\n",
    "                       random_state: int = 2025) -> Dict:\n",
    "    \"\"\"\n",
    "    hyperparameter tuning for random forest using randomised search\n",
    "    \n",
    "    parameter explanations:\n",
    "    - n_estimators: number of trees in forest (more trees = better performance but slower)\n",
    "    - max_depth: maximum tree depth (None = unlimited, higher = more complex, risk overfitting)\n",
    "    - min_samples_split: minimum samples required to split node (higher = more conservative splits)\n",
    "    - min_samples_leaf: minimum samples required at leaf node (higher = smoother decision boundaries)\n",
    "    - max_features: number of features to consider for best split ('sqrt' = square root of total features)\n",
    "    - bootstrap: whether to use bootstrap samples (True = sample with replacement)\n",
    "    \n",
    "    args:\n",
    "        X_train: training features\n",
    "        y_train: training target\n",
    "        n_iter: number of parameter combinations to try\n",
    "        cv: cross-validation folds\n",
    "        random_state: random seed\n",
    "        \n",
    "    returns:\n",
    "        dictionary with best parameters, best score, and fitted model\n",
    "    \"\"\"\n",
    "    # define parameter distributions\n",
    "    param_distributions = {\n",
    "        'n_estimators': randint(100, 500),           # uniform int between 100-500\n",
    "        'max_depth': [10, 20, 30, None],             # discrete choices\n",
    "        'min_samples_split': randint(50, 200),       # uniform int between 50-200\n",
    "        'min_samples_leaf': randint(25, 100),        # uniform int between 25-100\n",
    "        'max_features': ['sqrt', 'log2', None],      # feature selection strategies\n",
    "        'bootstrap': [True],                          # always use bootstrap\n",
    "        'class_weight': ['balanced']                  # handle class imbalance\n",
    "    }\n",
    "    \n",
    "    # base model\n",
    "    rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "    \n",
    "    # randomised search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        verbose=2,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(f\"starting randomised search with {n_iter} iterations...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nbest auc: {random_search.best_score_:.4f}\")\n",
    "    print(f\"best parameters: {random_search.best_params_}\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': random_search.best_params_,\n",
    "        'best_score': random_search.best_score_,\n",
    "        'best_model': random_search.best_estimator_,\n",
    "        'cv_results': pd.DataFrame(random_search.cv_results_)\n",
    "    }\n",
    "\n",
    "# execute tuning\n",
    "rf_tuning_results = tune_random_forest(X_train, y_train, n_iter=10, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5b5db-3e51-4175-9263-5857d9cac328",
   "metadata": {},
   "source": [
    "**Key Parameter Explanations**:\n",
    "\n",
    "- **n_estimators**: Number of decision trees in the ensemble. More trees generally improve performance through better averaging but increase computational cost. Typical range: 100-500.\n",
    "\n",
    "- **max_depth**: Maximum depth of each tree. Controls model complexity:\n",
    "  - Shallow trees (5-10): prevent overfitting, faster training, may underfit\n",
    "  - Medium trees (10-30): balanced approach for most problems\n",
    "  - Deep trees (>30 or None): capture complex patterns, risk overfitting\n",
    "\n",
    "- **min_samples_split**: Minimum samples required to split an internal node. Higher values:\n",
    "  - Prevent overfitting by making splits more conservative\n",
    "  - Create simpler trees with fewer nodes\n",
    "  - Typical range: 2-200 depending on dataset size\n",
    "\n",
    "- **min_samples_leaf**: Minimum samples required at leaf node. Higher values:\n",
    "  - Smooth decision boundaries\n",
    "  - Prevent overfitting to noise\n",
    "  - Must be < min_samples_split\n",
    "\n",
    "- **max_features**: Number of features considered for best split:\n",
    "  - 'sqrt': square root of total features (good default for classification)\n",
    "  - 'log2': log base 2 of total features (more conservative)\n",
    "  - None: consider all features (higher variance between trees)\n",
    "\n",
    "- **RandomizedSearchCV parameters**:\n",
    "  - **n_iter**: number of parameter combinations to sample\n",
    "  - **cv**: number of cross-validation folds\n",
    "  - **verbose**: controls output verbosity (0=silent, 1=progress, 2=detailed)\n",
    "  - **return_train_score**: whether to compute training scores (useful for diagnosing overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b4c6a-d7ed-4551-b7a3-5ea68733c535",
   "metadata": {},
   "source": [
    "### Logistic Regression Hyperparameter Tuning\n",
    "Optimise logistic regression with focus on regularisation and solver selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71641a4f-5e42-4947-80ad-13a2f097a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting grid search...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "best auc: 0.7107\n",
      "best parameters: {'C': 0.01, 'class_weight': 'balanced', 'max_iter': 5000, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_logistic_regression(X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                             cv: int = 5,\n",
    "                             random_state: int = 2025) -> Dict:\n",
    "    \"\"\"\n",
    "    hyperparameter tuning for logistic regression using grid search\n",
    "    \n",
    "    parameter explanations:\n",
    "    - C: inverse of regularisation strength (smaller = stronger regularisation)\n",
    "    - penalty: regularisation type ('l1' = lasso, 'l2' = ridge, 'elasticnet' = combination)\n",
    "    - solver: optimisation algorithm ('lbfgs' = limited-memory bfgs, 'saga' = stochastic average gradient)\n",
    "    - max_iter: maximum iterations for convergence\n",
    "    - class_weight: handling class imbalance\n",
    "    \n",
    "    args:\n",
    "        X_train: training features (must be scaled)\n",
    "        y_train: training target\n",
    "        cv: cross-validation folds\n",
    "        random_state: random seed\n",
    "        \n",
    "    returns:\n",
    "        dictionary with tuning results\n",
    "    \"\"\"\n",
    "    # parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],        # regularisation strength\n",
    "        'penalty': ['l2'],                           # l2 regularisation (ridge)\n",
    "        'solver': ['lbfgs', 'saga'],                 # optimisation algorithms\n",
    "        'max_iter': [5000],                          # sufficient for convergence\n",
    "        'class_weight': ['balanced']                 # handle imbalance\n",
    "    }\n",
    "    \n",
    "    # base model\n",
    "    lr = LogisticRegression(random_state=random_state)\n",
    "    \n",
    "    # grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lr,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(\"starting grid search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nbest auc: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_model': grid_search.best_estimator_,\n",
    "        'cv_results': pd.DataFrame(grid_search.cv_results_)\n",
    "    }\n",
    "\n",
    "# note: ensure X_train is scaled before using logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_tuning_results = tune_logistic_regression(\n",
    "    pd.DataFrame(X_train_scaled, columns=X_train.columns), \n",
    "    y_train, \n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3bc4b-5156-4aad-8bf7-5a49e9794a7b",
   "metadata": {},
   "source": [
    "**Key Parameter Explanations**:\n",
    "\n",
    "- **C (Regularisation Parameter)**:\n",
    "  - Inverse of regularisation strength (C = 1/λ)\n",
    "  - Small C (e.g., 0.001): strong regularisation, simple model, may underfit\n",
    "  - Large C (e.g., 100): weak regularisation, complex model, may overfit\n",
    "  - Default C=1.0 is often reasonable starting point\n",
    "\n",
    "- **penalty (Regularisation Type)**:\n",
    "  - **'l1' (Lasso)**: sum of absolute weights, creates sparse models (some weights = 0)\n",
    "  - **'l2' (Ridge)**: sum of squared weights, shrinks all weights but rarely to zero\n",
    "  - **'elasticnet'**: combination of l1 and l2, requires additional l1_ratio parameter\n",
    "  - Choice depends on feature selection needs and model interpretability requirements\n",
    "\n",
    "- **solver (Optimisation Algorithm)**:\n",
    "  - **'lbfgs'**: limited-memory bfgs, good for small datasets, only supports l2\n",
    "  - **'saga'**: stochastic average gradient, good for large datasets, supports all penalties\n",
    "  - **'liblinear'**: good for small datasets, supports l1 and l2\n",
    "  - **'newton-cg'**: newton-conjugate gradient, good for multinomial problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5886fa-2868-4fc1-b15b-988413c5b6c3",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance\n",
    "Apply Synthetic Minority Over-sampling Technique to balance classes.\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique) Explained**:\n",
    "\n",
    "SMOTE creates synthetic examples of the minority class rather than simply duplicating existing examples. The algorithm:\n",
    "\n",
    "1. **Select minority class sample**: Choose a random sample from minority class\n",
    "2. **Find k-nearest neighbours**: Identify k nearest minority class neighbours (typically k=5)\n",
    "3. **Generate synthetic sample**: \n",
    "   - Randomly select one of the k neighbours\n",
    "   - Create new sample along the line segment connecting the original sample and selected neighbour\n",
    "   - Formula: `synthetic = original + λ × (neighbour - original)` where λ ∈ [0, 1]\n",
    "4. **Repeat**: Generate synthetic samples until desired balance achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b699e1d-2e45-4d60-bc42-28dd0a6e0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original class distribution:\n",
      "Counter({0: 166702, 1: 44908})\n",
      "imbalance ratio: 3.71:1\n",
      "\n",
      "resampled class distribution:\n",
      "Counter({0: 166702, 1: 83351})\n",
      "new imbalance ratio: 2.00:1\n",
      "added 38443 synthetic samples\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def apply_smote(X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                sampling_strategy: float = 0.5,\n",
    "                k_neighbors: int = 5,\n",
    "                random_state: int = 2025) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    apply smote to handle class imbalance\n",
    "    \n",
    "    parameter explanations:\n",
    "    - sampling_strategy: desired ratio of minority to majority class after resampling\n",
    "      * float (e.g., 0.5): minority will be 50% of majority\n",
    "      * 'auto': resample minority to match majority (1:1 ratio)\n",
    "      * dict: specify exact counts for each class\n",
    "    \n",
    "    - k_neighbors: number of nearest neighbours for synthetic sample generation\n",
    "      * typical: 5 (default)\n",
    "      * smaller k: synthetic samples closer to original\n",
    "      * larger k: more diverse synthetic samples, risk of overlap with majority class\n",
    "    \n",
    "    - random_state: ensures reproducible synthetic sample generation\n",
    "    \n",
    "    args:\n",
    "        X_train: training features\n",
    "        y_train: training target\n",
    "        sampling_strategy: target ratio of minority to majority\n",
    "        k_neighbors: neighbours for synthetic generation\n",
    "        random_state: random seed\n",
    "        \n",
    "    returns:\n",
    "        resampled X_train and y_train\n",
    "    \"\"\"\n",
    "    print(\"original class distribution:\")\n",
    "    print(Counter(y_train))\n",
    "    print(f\"imbalance ratio: {(y_train==0).sum() / (y_train==1).sum():.2f}:1\")\n",
    "    \n",
    "    # apply smote\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        k_neighbors=k_neighbors,\n",
    "        random_state=random_state,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nresampled class distribution:\")\n",
    "    print(Counter(y_resampled))\n",
    "    print(f\"new imbalance ratio: {(y_resampled==0).sum() / (y_resampled==1).sum():.2f}:1\")\n",
    "    print(f\"added {len(y_resampled) - len(y_train)} synthetic samples\")\n",
    "    \n",
    "    return pd.DataFrame(X_resampled, columns=X_train.columns), pd.Series(y_resampled)\n",
    "\n",
    "# apply smote\n",
    "X_train_smote, y_train_smote = apply_smote(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    sampling_strategy=0.5,  # minority will be 50% of majority\n",
    "    k_neighbors=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74977a-df4b-4199-9853-164297d5be54",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison\n",
    "Systematically compare all tuned models on multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7848cfb-5777-43cd-bc2a-073407b4ae07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Check what Random Forest is actually predicting\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The tuning object has the best model in .best_estimator_\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m rf_predictions \u001b[38;5;241m=\u001b[39m lr_tuning_results\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique RF predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(rf_predictions, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual test labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y_test, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# 1. Check what Random Forest is actually predicting\n",
    "# The tuning object has the best model in .best_estimator_\n",
    "rf_predictions = lr_tuning_results.best_estimator_.predict(X_test)\n",
    "print(\"Unique RF predictions:\", np.unique(rf_predictions, return_counts=True))\n",
    "print(\"Actual test labels:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "# 2. Check the hyperparameters used\n",
    "print(\"\\nRF hyperparameters:\", rf_tuned.get_params())\n",
    "\n",
    "# 3. Check if it's predicting probabilities correctly\n",
    "rf_proba = rf_tuned.predict_proba(X_test)\n",
    "print(\"\\nProbability distribution:\")\n",
    "print(f\"Min: {rf_proba.min()}, Max: {rf_proba.max()}\")\n",
    "print(f\"Sample probabilities:\\n{rf_proba[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c5363f-f562-4f84-b789-93502ff5ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model comparison on test set:\n",
      "   model  roc_auc  precision   recall       f1\n",
      "lr_tuned 0.716112    0.34765 0.650213 0.453061\n",
      "rf_tuned 0.597937    0.00000 0.000000 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "def comprehensive_model_evaluation(models_dict: Dict,\n",
    "                                   X_test: pd.DataFrame,\n",
    "                                   y_test: pd.Series,\n",
    "                                   threshold: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    evaluate multiple models on test set with comprehensive metrics\n",
    "    \n",
    "    metrics explained:\n",
    "    - roc_auc: discriminative ability across all thresholds\n",
    "    - precision: proportion of positive predictions that are correct\n",
    "    - recall: proportion of actual positives correctly identified\n",
    "    - f1: harmonic mean balancing precision and recall\n",
    "    - specificity: proportion of actual negatives correctly identified\n",
    "    - false positive rate: proportion of negatives incorrectly classified as positive\n",
    "    \n",
    "    args:\n",
    "        models_dict: dictionary of fitted models\n",
    "        X_test: test features\n",
    "        y_test: test target\n",
    "        threshold: classification threshold for binary predictions\n",
    "        \n",
    "    returns:\n",
    "        comparison dataframe\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        # predictions\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        # metrics\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'specificity': tn / (tn + fp),\n",
    "            'fpr': fp / (fp + tn),\n",
    "            'threshold': threshold,\n",
    "            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('roc_auc', ascending=False)\n",
    "    \n",
    "    print(\"model comparison on test set:\")\n",
    "    print(results_df[['model', 'roc_auc', 'precision', 'recall', 'f1']].to_string(index=False))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# evaluate all tuned models\n",
    "all_models = {\n",
    "    'lr_tuned': lr_tuning_results['best_model'],\n",
    "    'rf_tuned': rf_tuning_results['best_model'],\n",
    "\n",
    "}\n",
    "\n",
    "comparison_results = comprehensive_model_evaluation(\n",
    "    all_models,\n",
    "    X_test_scaled,  # use scaled features for models that need it\n",
    "    y_test,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130e4ac-66ae-4ed5-8cb9-4b1b27cb4bac",
   "metadata": {},
   "source": [
    "## Learning Curves for Diagnosing Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2f4313-61c3-4c5e-a77b-d76e0988b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(model, X: pd.DataFrame, y: pd.Series,\n",
    "                        cv: int = 5,\n",
    "                        train_sizes: np.ndarray = np.linspace(0.1, 1.0, 10)) -> None:\n",
    "    \"\"\"\n",
    "    generate learning curves to diagnose bias/variance\n",
    "    \n",
    "    interpretation:\n",
    "    - high train score, low validation score: overfitting (high variance)\n",
    "    - both scores low: underfitting (high bias)\n",
    "    - both scores high and close: good fit\n",
    "    - scores converging with more data: model will benefit from more data\n",
    "    - large gap persists: model complexity issue, not data issue\n",
    "    \n",
    "    args:\n",
    "        model: sklearn estimator\n",
    "        X: features\n",
    "        y: target\n",
    "        cv: cross-validation folds\n",
    "        train_sizes: fractions of training set to use\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "        train_sizes=train_sizes,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='training score', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, val_mean, label='validation score', marker='s')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "    \n",
    "    plt.xlabel('training set size')\n",
    "    plt.ylabel('roc auc score')\n",
    "    plt.title('learning curves')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # diagnosis\n",
    "    final_gap = train_mean[-1] - val_mean[-1]\n",
    "    if final_gap > 0.1:\n",
    "        print(\"diagnosis: overfitting detected (train >> validation)\")\n",
    "        print(\"recommendations: increase regularisation, reduce model complexity, or gather more data\")\n",
    "    elif val_mean[-1] < 0.7:\n",
    "        print(\"diagnosis: underfitting detected (both scores low)\")\n",
    "        print(\"recommendations: increase model complexity, add features, or reduce regularisation\")\n",
    "    else:\n",
    "        print(\"diagnosis: good fit (scores high and close)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb41ef1-a7ae-442f-a1b2-c0204a9ef5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
