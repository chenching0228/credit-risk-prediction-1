{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c8f547-7bf4-4502-963a-9b6bc081fcae",
   "metadata": {},
   "source": [
    "## 3.1 Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027d64a6-e8ff-4b30-abcf-c79dc8ad14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 211,610 samples\n",
      "validation set: 45,345 samples\n",
      "test set: 45,346 samples\n",
      "features: 26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 2025\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def prepare_modelling_data(df: pd.DataFrame, target: str = 'target') -> Tuple:\n",
    "    \"\"\"\n",
    "    prepare data for model training with proper encoding and splitting\n",
    "    \n",
    "    args:\n",
    "        df: preprocessed dataframe\n",
    "        target: target variable name\n",
    "        \n",
    "    returns:\n",
    "        tuple of (X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders)\n",
    "    \"\"\"\n",
    "    df_model_prep = df.copy()\n",
    "    \n",
    "    # separate features and target\n",
    "    if target not in df_model_prep.columns:\n",
    "        raise ValueError(f\"target column '{target}' not found in dataframe\")\n",
    "    \n",
    "    y = df_model_prep[target]\n",
    "    X = df_model_prep.drop(columns=[target])\n",
    "    \n",
    "    # encode categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    # convert datetime columns to numeric\n",
    "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    for col in datetime_cols:\n",
    "        X[f'{col}_year'] = X[col].dt.year\n",
    "        X[f'{col}_month'] = X[col].dt.month\n",
    "        X = X.drop(columns=[col])\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # split data: 70% train, 15% validation, 15% test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"train set: {len(X_train):,} samples\")\n",
    "    print(f\"validation set: {len(X_val):,} samples\")\n",
    "    print(f\"test set: {len(X_test):,} samples\")\n",
    "    print(f\"features: {len(feature_names)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders\n",
    "\n",
    "# load data\n",
    "df_model_processed = pd.read_csv(\"/Users/chenjing/Desktop/credit-risk-prediction/data/processed/accepted_loans_model_ready.csv\")\n",
    "\n",
    "\n",
    "# prepare data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_names, encoders = prepare_modelling_data(\n",
    "    df_model_processed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7fde5-f96b-461a-95b4-93fe062d13fd",
   "metadata": {},
   "source": [
    "## 3.2 Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d5a131-6ca1-48a8-bdec-27b479ecbc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      "applying SMOTE for class balance...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, scaler, y_val_pred_proba, metrics\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# train logistic regression\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m lr_model, lr_scaler, lr_pred_proba, lr_metrics \u001b[38;5;241m=\u001b[39m train_logistic_regression(\n\u001b[1;32m    126\u001b[0m     X_train, y_train, X_val, y_val, use_smote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    127\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(X_train, y_train, X_val, y_val, use_smote)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mapplying SMOTE for class balance...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[0;32m---> 23\u001b[0m     X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train_balanced)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/imblearn/base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/imblearn/base.py:99\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     97\u001b[0m check_classification_targets(y)\n\u001b[1;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m---> 99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/imblearn/base.py:157\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    155\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    156\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 157\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1371\u001b[0m     X,\n\u001b[1;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1373\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1374\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1375\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1376\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1377\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[1;32m   1378\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39mensure_all_finite,\n\u001b[1;32m   1379\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1380\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1381\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1382\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1383\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1385\u001b[0m )\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1108\u001b[0m         array,\n\u001b[1;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_val, y_val, use_smote: bool = True):\n",
    "    \"\"\"\n",
    "    train and evaluate logistic regression model\n",
    "    \n",
    "    args:\n",
    "        X_train: training features\n",
    "        y_train: training target\n",
    "        X_val: validation features\n",
    "        y_val: validation target\n",
    "        use_smote: whether to apply SMOTE for class balance\n",
    "        \n",
    "    returns:\n",
    "        trained model, scaler, predictions, metrics\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LOGISTIC REGRESSION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # handle class imbalance with SMOTE if requested\n",
    "    if use_smote:\n",
    "        print(\"\\napplying SMOTE for class balance...\")\n",
    "        smote = SMOTE(random_state=SEED)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"balanced training set: {len(X_train_balanced):,} samples\")\n",
    "    else:\n",
    "        X_train_balanced = X_train\n",
    "        y_train_balanced = y_train\n",
    "    \n",
    "    # scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # train model\n",
    "    print(\"\\ntraining logistic regression...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=SEED,\n",
    "        class_weight='balanced',\n",
    "        solver='lbfgs',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    \n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # evaluation\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'accuracy': accuracy_score(y_train_balanced, y_train_pred),\n",
    "            'precision': precision_score(y_train_balanced, y_train_pred),\n",
    "            'recall': recall_score(y_train_balanced, y_train_pred),\n",
    "            'f1': f1_score(y_train_balanced, y_train_pred),\n",
    "            'roc_auc': roc_auc_score(y_train_balanced, y_train_pred_proba)\n",
    "        },\n",
    "        'validation': {\n",
    "            'accuracy': accuracy_score(y_val, y_val_pred),\n",
    "            'precision': precision_score(y_val, y_val_pred),\n",
    "            'recall': recall_score(y_val, y_val_pred),\n",
    "            'f1': f1_score(y_val, y_val_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_val_pred_proba)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # print results\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"TRAINING SET PERFORMANCE\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric, value in metrics['train'].items():\n",
    "        print(f\"{metric:.<20} {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"VALIDATION SET PERFORMANCE\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric, value in metrics['validation'].items():\n",
    "        print(f\"{metric:.<20} {value:.4f}\")\n",
    "    \n",
    "    # confusion matrix\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"CONFUSION MATRIX (Validation)\")\n",
    "    print(\"-\" * 70)\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    print(f\"true negatives:  {cm[0, 0]:>8,}\")\n",
    "    print(f\"false positives: {cm[0, 1]:>8,}\")\n",
    "    print(f\"false negatives: {cm[1, 0]:>8,}\")\n",
    "    print(f\"true positives:  {cm[1, 1]:>8,}\")\n",
    "    \n",
    "    # visualise confusion matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title('confusion matrix')\n",
    "    axes[0].set_xlabel('predicted')\n",
    "    axes[0].set_ylabel('actual')\n",
    "    axes[0].set_xticklabels(['non-default', 'default'])\n",
    "    axes[0].set_yticklabels(['non-default', 'default'])\n",
    "    \n",
    "    # roc curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f\"auc = {metrics['validation']['roc_auc']:.4f}\", linewidth=2)\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', label='random classifier')\n",
    "    axes[1].set_xlabel('false positive rate')\n",
    "    axes[1].set_ylabel('true positive rate')\n",
    "    axes[1].set_title('roc curve')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, scaler, y_val_pred_proba, metrics\n",
    "\n",
    "# train logistic regression\n",
    "lr_model, lr_scaler, lr_pred_proba, lr_metrics = train_logistic_regression(\n",
    "    X_train, y_train, X_val, y_val, use_smote=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fac59-0c70-403b-96d3-af523d8df85d",
   "metadata": {},
   "source": [
    "## 3.3 Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef77738-72ec-4765-9076-6ece6ddc8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANDOM FOREST\n",
      "======================================================================\n",
      "\n",
      "training random forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, y_val_pred_proba, metrics\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# train random forest\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m rf_model, rf_pred_proba, rf_metrics \u001b[38;5;241m=\u001b[39m train_random_forest(\n\u001b[1;32m    124\u001b[0m     X_train, y_train, X_val, y_val, use_smote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    125\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mtrain_random_forest\u001b[0;34m(X_train, y_train, X_val, y_val, use_smote)\u001b[0m\n\u001b[1;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     34\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     35\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_balanced, y_train_balanced)\n\u001b[1;32m     45\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/ensemble/_forest.py:375\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[1;32m    374\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 375\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(\n\u001b[1;32m    376\u001b[0m         X, estimator_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/tree/_classes.py:222\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[0;34m(self, X, estimator_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m     overall_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(X)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(overall_sum):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     _assert_all_finite_element_wise(X, xp\u001b[38;5;241m=\u001b[39mnp, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# If the sum is not nan, then there are no missing values\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(overall_sum):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "def train_random_forest(X_train, y_train, X_val, y_val, use_smote: bool = False):\n",
    "    \"\"\"\n",
    "    train and evaluate random forest classifier\n",
    "    \n",
    "    args:\n",
    "        X_train: training features\n",
    "        y_train: training target\n",
    "        X_val: validation features\n",
    "        y_val: validation target\n",
    "        use_smote: whether to apply SMOTE (not typically needed for RF)\n",
    "        \n",
    "    returns:\n",
    "        trained model, predictions, metrics\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RANDOM FOREST\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # handle class imbalance with SMOTE if requested\n",
    "    if use_smote:\n",
    "        print(\"\\napplying SMOTE for class balance...\")\n",
    "        smote = SMOTE(random_state=SEED)\n",
    "        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"balanced training set: {len(X_train_balanced):,} samples\")\n",
    "    else:\n",
    "        X_train_balanced = X_train\n",
    "        y_train_balanced = y_train\n",
    "    \n",
    "    # train model\n",
    "    print(\"\\ntraining random forest...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # predictions\n",
    "    y_train_pred = model.predict(X_train_balanced)\n",
    "    y_train_pred_proba = model.predict_proba(X_train_balanced)[:, 1]\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # evaluation\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'accuracy': accuracy_score(y_train_balanced, y_train_pred),\n",
    "            'precision': precision_score(y_train_balanced, y_train_pred),\n",
    "            'recall': recall_score(y_train_balanced, y_train_pred),\n",
    "            'f1': f1_score(y_train_balanced, y_train_pred),\n",
    "            'roc_auc': roc_auc_score(y_train_balanced, y_train_pred_proba)\n",
    "        },\n",
    "        'validation': {\n",
    "            'accuracy': accuracy_score(y_val, y_val_pred),\n",
    "            'precision': precision_score(y_val, y_val_pred),\n",
    "            'recall': recall_score(y_val, y_val_pred),\n",
    "            'f1': f1_score(y_val, y_val_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_val_pred_proba)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # print results\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"TRAINING SET PERFORMANCE\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric, value in metrics['train'].items():\n",
    "        print(f\"{metric:.<20} {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"VALIDATION SET PERFORMANCE\")\n",
    "    print(\"-\" * 70)\n",
    "    for metric, value in metrics['validation'].items():\n",
    "        print(f\"{metric:.<20} {value:.4f}\")\n",
    "    \n",
    "    # confusion matrix\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"CONFUSION MATRIX (Validation)\")\n",
    "    print(\"-\" * 70)\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    print(f\"true negatives:  {cm[0, 0]:>8,}\")\n",
    "    print(f\"false positives: {cm[0, 1]:>8,}\")\n",
    "    print(f\"false negatives: {cm[1, 0]:>8,}\")\n",
    "    print(f\"true positives:  {cm[1, 1]:>8,}\")\n",
    "    \n",
    "    # visualise confusion matrix and roc curve\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # confusion matrix heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[0])\n",
    "    axes[0].set_title('confusion matrix')\n",
    "    axes[0].set_xlabel('predicted')\n",
    "    axes[0].set_ylabel('actual')\n",
    "    axes[0].set_xticklabels(['non-default', 'default'])\n",
    "    axes[0].set_yticklabels(['non-default', 'default'])\n",
    "    \n",
    "    # roc curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba)\n",
    "    axes[1].plot(fpr, tpr, label=f\"auc = {metrics['validation']['roc_auc']:.4f}\", linewidth=2)\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', label='random classifier')\n",
    "    axes[1].set_xlabel('false positive rate')\n",
    "    axes[1].set_ylabel('true positive rate')\n",
    "    axes[1].set_title('roc curve')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, y_val_pred_proba, metrics\n",
    "\n",
    "# train random forest\n",
    "rf_model, rf_pred_proba, rf_metrics = train_random_forest(\n",
    "    X_train, y_train, X_val, y_val, use_smote=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f85496-9a80-48be-b09b-e95c2edaee7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
